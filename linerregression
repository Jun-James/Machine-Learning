# -*- coding: utf-8 -*-
"""
Created on Mon Feb 10 20:13:49 2020

@author: boyu
"""

import numpy as np
from sklearn import preprocessing

# 加载数据
lines=np.loadtxt('D:\spyder\linerregression/USA_Housing.csv', delimiter=',', dtype='str')
print("输入")
print(lines)
for i in range(lines.shape[1]-1):
    print(lines[0, i])
print("标签")
print(lines[0,-1])

x_total = lines[1:, :5].astype('float')
y_total = lines[1:, 5:].astype('float').flatten() # flatten 拉平成一个行向量
#print(y_total)

# 数据预处理和切分
x_total = preprocessing.scale(x_total)   #将数缩放到（-1,1）之间以让训练更稳定 
#print(x_total)
#print(x_total.mean())
y_total = preprocessing.scale(y_total)
x_train = x_total[:4000]   # 
x_test = x_total[4000:]
y_train = y_total[:4000]
y_test = y_total[4000:]
print('训练集大小: ', x_train.shape[0])
print('测试集大小: ', x_test.shape[0])

# Normal Equation
X_train = np.hstack([x_train, np.ones((x_train.shape[0], 1))])   #水平方向平铺  np.vstack（）竖直方向堆栈
#print(X_train)
NE_solution = np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(X_train), X_train)), np.transpose(X_train)), y_train.reshape([-1, 1]))
print(NE_solution)

X_test = np.hstack([x_test, np.ones((x_test.shape[0], 1))])
y_pred_test = np.dot(X_test, NE_solution).flatten()

rmse_loss = np.sqrt(np.square(y_test - y_pred_test).mean())
print('rmse_loss:', rmse_loss)

# Sklearn库
from sklearn import linear_model

linreg = linear_model.LinearRegression()
linreg.fit(x_train, y_train)
print(linreg.coef_)
print(linreg.intercept_)
y_pred_test = linreg.predict(x_test)

rmse_loss = np.sqrt(np.square(y_test - y_pred_test).mean())
print('rmse_loss:', rmse_loss)

# Gradient Descent
#shuffle_index = np.random.permutation(4000)
#print(shuffle_index)      # 0-3999的数
def shuffle_aligned_list(data):
    num = data[0].shape[0]
    shuffle_index = np.random.permutation(num)
    return [d[shuffle_index] for d in data]
# 必须用同一个shuffle_index进行打乱

def batch_generator(data, batch_size, shuffle=True):  #batch_size 每次取的数据的个数
    batch_count = 0   #取的batch的数量
    while True:
        if batch_count * batch_size + batch_size >= data[0].shape[0]:
            batch_count = 0
            if shuffle:
                data = shuffle_aligned_list(data)
        start = batch_count * batch_size
        end = start + batch_size
        batch_count += 1
        yield [d[start:end] for d in data]
        
# 具体的用梯度下降法进行训练的代码
import matplotlib.pyplot as plt   # matplotlib 用来画图的
num_steps = 600   #一共进行600步梯度下降
learning_rate = 0.01  #梯度下降学习速率
batch_size = 40    #每次用多少个数据来训练

weight = np.zeros(6)   #初始化
np.random.seed(0)      #设置一个随机种子
batch_g = batch_generator([x_train, y_train], batch_size, shuffle=True)
x_test_concat = np.hstack([x_test, np.ones([x_test.shape[0], 1])])

loss_list = []
for i in range(num_steps):
    rmse_loss = np.sqrt(np.square(np.dot(x_test_concat, weight) - y_test).mean())
    loss_list.append(rmse_loss)
    
    x_batch, y_batch = batch_g.__next__()
    x_batch = np.hstack([x_batch, np.ones([batch_size, 1])])
    y_pred = np.dot(x_batch, weight)
    w_gradient = (x_batch * np.tile((y_pred - y_batch).reshape([-1, 1]), 6)).mean(axis=0)
    weight = weight - learning_rate * w_gradient 

# a = np.array([1,2,3])
# a = a.reshape([-1,1])
# b = np.tile(a,6)  #把a复制6份
# print(a,b)
    
print('weight:', weight)
print('rmse_loss:', rmse_loss)
    
loss_array = np.array(loss_list)
plt.plot(np.arange(num_steps), loss_array)
plt.show()        
